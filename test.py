import re
from tomark import Tomark
import numpy as np
import json

basic_info = {"ä¸­æ–‡å": "è´é©¬çº¦å°”", "å¤–æ–‡å": "FranzBurgmeier", "å›½ç±": "åˆ—æ”¯æ•¦å£«ç™»", "å‡ºç”Ÿæ—¥æœŸ": "1982å¹´4æœˆ7æ—¥", "èº«é«˜": "177 cm", "ä½“é‡": "73 kg", "è¿åŠ¨é¡¹ç›®": "è¶³çƒ", "æ‰€å±è¿åŠ¨é˜Ÿ": "ç“¦æœå…¹"}

markdown = Tomark.table([basic_info])



cleaned_content="[è½¬è½½]æ¸©å®¶å®1.2:å°†é€šè¿‡é€‚å½“æ¸ é“å‚ä¸è§£å†³æ¬§å€ºé—®é¢˜"
cleaned_content = re.sub(r'([\u4e00-\u9fa5])[.*?]',r'\1ã€‚', cleaned_content)


cleaned_content="åŸæ–‡åœ°å€:å¤ä»£è§‚äººç§˜æœ¯ä½œè€…:é’å²›åŸé˜³å¾‹å¸ˆ 1ã€å¤´å‘å¤šçš„äººæ˜¯åŠ³ç¢Œå,å¿ƒçœ¼å°ã€‚"
cleaned_content = re.sub(r'^[^\s]*\s',"",cleaned_content)




import os

def getFlist(file_dir):
    for root, dirs, files in os.walk(file_dir):
        print('root_dir:', root)
        print('files:', files)

        print("="*60)
    return files

#resDir = '/data/data_warehouse/llm/source_data/cn-kindle/'
#flist = getFlist(resDir)


import json
import gzip
input_file="/data/data_warehouse/llm/source_data/cn-mnbvc/law/judgement/20230134/1.jsonl.gz"
input_file="/data/data_warehouse/llm/source_data/cn-mnbvc/gov/20230172/XueXiQiangGuo.jsonl.gz"
input_file="/data/data_warehouse/llm/source_data/cn-mnbvc/gov/20230172/GovReport.jsonl.gz"
input_file="/data/data_warehouse/llm/source_data/cn-mnbvc/qa/20230196/wikihow/wikihow_zh.0.jsonl.gz"
'''
with gzip.open(input_file, 'rt') as f:
    for line in f:
        js_dict = json.loads(line)
        print(js_dict)
'''


a="## è–›ä¸‹æ‘ä¹¡ \nè–›ä¸‹æ‘ä¹¡æ˜¯ä¸­å›½é™•è¥¿çœæ¦†æ—å¸‚å´å ¡å¿ä¸‹è¾–çš„ä¸€ä¸ªä¹¡ã€‚\n## è¡Œæ”¿åŒºåˆ’ \nè–›ä¸‹æ‘ä¹¡ä¸‹è¾–ä»¥ä¸‹è¡Œæ”¿åŒºï¼š\nè–›ä¸‹æ‘ã€å‰å±±æ‘ã€åå±±æ‘ã€æ°´æ¸¸æ‘ã€æ¨ªæ²Ÿæ‘ã€å¯¨å±±æ‘ã€å®‰å®¶å±±æ‘ã€å¤§çŸ³å¬æ‘ã€å—å±±ä¸Šæ‘ã€æå®¶åœªå´‚æ‘ã€ä¸œç‹å®¶å±±æ‘ã€æ­¦å®¶å±±æ‘ã€ç –çª‘å±±æ‘ã€ç•”ç•”å±±æ‘ã€æå®¶æ²Ÿæ‘ã€æ§æ ‘æ¸¯æ‘ã€æ£ä¸°æ ‘æ‘ã€å‡†åˆ™å±±æ‘ã€ç»­å®¶å¬æ‘ã€å°ç‹å®¶å±±æ‘ã€è–›ä¸Šæ‘ã€åº™å²”ä¸Šæ‘ã€å—å³ªåˆ™æ‘ã€å‰èƒ¡å®¶å±±æ‘ã€ç‹å®¶åœªå´‚æ‘ã€è¾›ç¤¾çª æ‘ã€åèƒ¡å®¶å±±æ‘"
#print(len(a))


def is_TablePage(text,mean_thresh=10.0,var_thresh=8.0,m=3.0):
    tokens = re.split(r'[\nã€]',text)
    conlen = np.array([len(item) for item in tokens])
    print("conlen:",conlen)
    me = np.mean(conlen)
    var = np.std(conlen,ddof=1)
    print("mean:",me)
    print("var:",var)
    conlen = conlen[abs(conlen - me) < m*var]
    me = np.mean(conlen)
    var = np.std(conlen,ddof=1)
    print("mean:",me)
    print("var:",var)
    if me < mean_thresh and var < var_thresh: return True
    return False

'''for line in open("/data/data_warehouse/llm/clean_data/cn-wiki/v6/cn-wiki-sample100-v6.jsonl"):
    if line.find("é£æ°´ä¸–å®¶") != -1:
        js_dict = json.loads(line)
        text = js_dict["content"]
        res = is_TablePage(text)
'''

import jieba
text="æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦æœ‰ä¹±ç "
text="äººçœ¼èƒ½è¯†åˆ«çš„ä¹±ç åœ¨ç¨‹åºçœ‹æ¥å¹¶æ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆç®€å•ã€‚é’ˆå¯¹ç¨‹åºæ¥æœ¬èº«ä¹Ÿæ˜¯æ­£å¸¸çš„å­—ç¬¦ã€‚ä¸‹é¢åˆ†äº«ä¸‹ä¸€äº›æŠ˜ä¸­çš„æ–¹æ¡ˆï¼šæ–¹æ¡ˆä¸€ï¼šå¯¹åˆ†è¯åçš„åˆ†è¯ç‡è¿›è¡Œç»Ÿè®¡ä»æ¦‚ç‡å±‚é¢ï¼Œæ­£å¸¸çš„æ–‡æœ¬åˆ†è¯ç‡ï¼ˆæ–‡æœ¬é•¿åº¦/åˆ†è¯åä¸ªæ•°ï¼‰>2ï¼Œè€Œä¹±ç å­—ç¬¦åˆ™æ¥è¿‘1ã€‚å…·ä½“ä»£ç å¦‚ä¸‹ï¼š"
text="æ¶“å›§å“é”›å±¾å›é”½å‹¬å´˜éš"
strlen = len(text)
seglen = len(jieba.lcut(text))
#print("strlen:",strlen)
#print("seglen:",seglen)
#print("ratio:",1.0*strlen/seglen)


# /data/data_warehouse/llm/source_data/cn-xhs

tokenizer_path="/data/pangwei/chinese_llama_13b_plus14/"
tokenizer_kwargs = {
        "use_fast": True,
        "revision": "productGPT"
}

from transformers import LlamaTokenizer
tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path, **tokenizer_kwargs)
tokenizer.pad_token = tokenizer.eos_token

content="æ²¡æœ‰è¶³å¤Ÿçš„ğŸ’°æ¥ä¹°è¿™ç§ä»ªå™¨"
tokens = tokenizer.encode(content)
num_tokens = len(tokens)


from clean_headtails_from_content import CleanHeadTailsFromContent

idx = 0
clean = CleanHeadTailsFromContent("./wechat_ads_phrase.txt")
for line in open("/root/llm/source_data/cn-wechat/wx_data_981.jsonl"):
    line = line.strip()
    if len(line) < 1: continue
    js_dict = json.loads(line)
    content = js_dict["content"]

    res = clean.clean(content)
    print("oldtext:",content)
    print("newtext:",res)
    print("=="*40)
    idx += 1
    if idx > 50: break







